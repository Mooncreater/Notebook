---
comments: true
---

## 第十章
<h2 style="color: #2931d9ff; font-weight: normal;"> 降维与度量学习</h2>

### 1.k近邻学习

要判断一个样本的类别，就看看它周围最近的K个邻居是什么类别，然后"随大流"。

出错率：$P(err) = 1- \sum\limits_{c \in \mathcal{Y}}P(c|\bm{x})P(c|\bm{z})$

![alt text](img/{0FAE27C8-E91F-47F9-A8AB-9A276E1A7EC8}.png)

最近邻分类器虽简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍

### 2.低维嵌入
在高维情形下出现的数据样本稀疏、距离计算困难等问题

***降维***
![alt text](img/{A8CF2018-1C0D-43A7-9637-5741621FCE43}.png)

多维缩放：
要求原始空间中样本之间的距离在低维空间中得以保持

![alt text](img/{8B76BAF9-7B6D-4151-BA69-5026F9DF3BA6}.png)

![alt text](img/{6B5F8EA0-E705-43AD-9375-514F9F74589A}.png)

由此即可通过降维前后保持不变的距离矩阵D求取内积矩阵B

### 3.主成分分析

对于正交属性空间中的样本点，如何用一个超平面(直线的高维推广)对所有样本进行恰当的表达?

- [x] 最近重构性：样本点到这个超平面的距离都足够近
- [x] 最大可分性：样本点在这个超平面上的投影能尽可能分开

**PCA**
输入： 样本集，低维空间维数
算法：
1. 对所有样本进行中心化
2. 计算样本的协方差矩阵 $X^TX$
3. 对协方差矩阵做特征值分解
4. 取最大的 $d'$ 个特征值所对应的特征向量 $w_1,\cdots,w_{d'}$

输出:投影矩阵 $\bm{W} = (\bm{w_1,\cdots,w_{d'}}).$

降维后低维空间的维数d′通常是由用户事先指定，或通过在d′值不同的低维空间中对k近邻分类器进行交叉验证来选取较好的d′值。对PCA，还可从重构的角度设置一个重构阔值，然后选取使下式成立的最小 $d'$ 值：
![alt text](img/{B81A8453-F535-4020-9D90-7FD0C9D393C8}.png)

$PCA $仅需保留 $W$与样本的均值向量即可通过简单的向量减法和矩阵-向量乘法将新样本投影至低维空间中

### 4. 核化线性降维

![alt text](img/{073FC50F-77AC-41BD-8352-D6DA8E36E7FA}.png)

![alt text](img/{47D24F09-6BE1-4EF0-8B16-2515CD0D0307}.png)

### 5.流形学习

