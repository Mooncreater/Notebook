## 第八章
<h2 style="color: #2931d9ff; font-weight: normal;"> 集成学习</h2>

#### 1.个体与集成
多个学习器，最后投票处最终结果
![alt text](img/{5AA07356-D087-40D9-840C-C46D33C2FBC4}.png)
- [x] 集成个体：好而不同

两大类：串行和并行

##### 1.1 串行-Boosting
每次调整训练数据的样本分布
下一次训练重点关照上一组错误率高的样本
组合的最终版各个学习器权重不一样(表现好者高)
![alt text](img/{5D1D30E6-6928-4981-A6C3-818291F38233}.png)

##### 1.2 AdaBoost算法
解决了什么？

- [x] 下一组训练的权重分配问题,即构造出了 $D_{t+1}$

![alt text](img/{C0DE2768-8D2E-40CC-8968-CC81F2488DAB}.png)

最终 $D_{t+1}$ 基于 $D_t$ 如下：
![alt text](img/{E9FDA2FE-AB07-4D7B-9771-A970A9B234FE}.png)

**注意事项：**
数据分布的学习：
- [x] 重赋权法
- [x] 重采样法

重启动，避免训练过程过早停止

偏差-方差：降低偏差，可对泛化性能相当弱的学习器构造出很强的集成

##### 1.3 并行-Bagging
- [x] 个体学习器不存在强依赖关系
- [x] 并行化生成
- [x] 自助采样法

##### 1.4 Bagging

**思路：** 通过构建多个独立的模型，让它们投票或平均，从而获得更稳定、更可靠的预测。

![alt text](img/{5D85E858-A61B-4EF7-BE55-7B262C12D352}.png)

1.从训练集 $D$ 中有放回地随机抽取 n 个样本，形成新的训练集 $D_t$

2.可使用包外预测：使用未采样的样本进行模型验证

##### 1.5 随机森林
**RF** 是bagging的拓展变种

从根节点开始，递归地：

1. **特征子集选择**：从 p 个特征中随机选择 m 个特征（通常 m = √p 或 log₂(p)）

2. **最佳分裂点选择**：从这 m 个特征中，选择最佳的分裂特征和分裂点（基于基尼指数或信息增益）

3. **节点分裂**：将数据划分为两个子集

4. **递归构建**：对每个子节点重复上述过程，直到：
    - 节点样本数小于阈值
    - 深度达到最大值
    - 节点纯度足够高

#### 2.结合策略
- [x] 平均法
- [x] 投票法
- [x] 学习法

#### 3.多样性 

##### 3.1 误差-分歧分解
![alt text](img/{17E058A5-867E-4912-852E-477D3E60C5AA}.png)

这个漂亮的式子显示:个体学习器精确性越高、多样性越大，则集成效果越好。称为误差-分歧分解

